{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating points from given condition\n",
    "t=np.zeros((21,2))\n",
    "t.shape\n",
    "for i in range(21):\n",
    "    x_i=-2+0.2*i\n",
    "    x_j=-2+0.2*i\n",
    "    t[i,0]=x_i\n",
    "    t[i,1]=x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a training set, use 441 randomly sampled data points\n",
    "data_rand=np.zeros((441,3))\n",
    "for i in range(441):\n",
    "    out=0\n",
    "    x_i=random.choice(t[:,0])\n",
    "    x_j=random.choice(t[:,1])\n",
    "    k_val=x_i*x_i+x_j*x_j\n",
    "    if k_val<=1:\n",
    "        out=+1\n",
    "    elif k_val>1:\n",
    "        out=-1\n",
    "    data_rand[i,0]=x_i\n",
    "    data_rand[i,1]=x_j\n",
    "    data_rand[i,2]=out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe conversion for plotting the data\n",
    "\n",
    "data=pd.DataFrame(data_rand,columns=['x_i','x_j','target'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.scatterplot(x='x_i',y='x_j', hue='target',data=data, palette=[\"C0\", \"C2\"])\n",
    "#commented because the TA warned about seaborn use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split in the ratio 8:2\n",
    "train_data=data.iloc[:352,:]\n",
    "test_data=data.iloc[352:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the ratio of positive and negative target in train and test set\n",
    "# to make sure dataset is balacnced\n",
    "def data_ratio_check(x):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i in range(len(x)):\n",
    "        if x.iloc[i, 2] == 1:\n",
    "            pos = pos + 1\n",
    "        elif x.iloc[i, 2] == -1:\n",
    "            neg = neg + 1\n",
    "    print('positive data points ',pos)\n",
    "    print('negative data points',neg)\n",
    "    print('Ratio ',neg/pos)\n",
    "    return neg/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the ratio of positive and negative target in traindata\n",
    "train_data_ratio=data_ratio_check(train_data)\n",
    "train_data_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the ratio of positive and negative target in test data\n",
    "test_data_ratio=data_ratio_check(test_data)\n",
    "test_data_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtrain & ytrain\n",
    "X_train=train_data[['x_i','x_j']].values\n",
    "y_train=train_data[['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest and ytest\n",
    "X_test=test_data[['x_i','x_j']].values\n",
    "y_test=test_data[['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the distance between the center of the radial funciton and inputs\n",
    "def dist(inp,hidden):\n",
    "    out_arr=np.zeros((1,len(inp)))\n",
    "    for i in range(len(hidden)):\n",
    "        val=np.linalg.norm(inp-hidden[i,],axis=1)\n",
    "        out_arr=np.vstack([out_arr,val])\n",
    "    out=out_arr[1:,]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the gaussian function given distance and spread\n",
    "def rbf(x, s):\n",
    "    return np.exp(-1 / (2 * s**2) * (x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining sigmoid funciton\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targer from -1 has been converted to 0 since I am using sigmoid function at output layer\n",
    "def label_conv(x):\n",
    "    for i in range(len(x)):\n",
    "        if x[i, 0] == -1:\n",
    "            x[i, 0] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forwad pass function\n",
    "def fwd(test,centers,weights,bias,sif):\n",
    "    inp_dist=dist(test,centers)\n",
    "    test=rbf(inp_dist,sif)\n",
    "    pred = np.dot(test.T, weights) + bias\n",
    "    pred=sigmoid(pred)\n",
    "    pred=np.round(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcition for finding out the accuracy\n",
    "def acc(y_prediction,y):\n",
    "    acc=0\n",
    "    for i in range(len(y_prediction)):\n",
    "        if y_prediction[i,0]==y[i,0]:\n",
    "            acc=acc+1\n",
    "    return (acc/len(y_prediction))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the labels of y train\n",
    "labels=label_conv(y_train)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of RBF NN based on Gaussian kernel functions with constant spread function(0.8) and using all the points in the training set as centers of the RB functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random weight generation\n",
    "weights = np.random.randn(352,1)#  Weight initialization\n",
    "bias = np.random.randn(1) # Bias initialization\n",
    "lr = 0.04 #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "        inp_dist=dist(X_train,X_train)# calculating distance from hidden layer neurons\n",
    "        feature_set=rbf(inp_dist,0.8)#passing it to the rbf function\n",
    "\n",
    "        # FeedForward\n",
    "        fw_pass = np.dot(feature_set, weights) + bias\n",
    "        z = sigmoid(fw_pass)\n",
    "        # Error calculation\n",
    "        error = z - labels\n",
    "        e1=(z-labels)*(z-labels)\n",
    "        # Backpropagation\n",
    "        weights -= lr * np.dot(feature_set.T, error)\n",
    "        for num in error:\n",
    "            bias -= lr * num\n",
    "\n",
    "        #Accuracy calculation using the test datset\n",
    "        y_pred=fwd(X_test,X_train,weights,bias,0.8)\n",
    "        a1=acc(y_pred,label_conv(y_test))\n",
    "        print('Epoch num :',epoch,' Spread value ',0.8,' Error :',(e1.sum())/352,' Accuracy :',a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of RBF NN based on Gaussian kernel functions with varing spread function(0.3, 0.8, 1.3) and using all the points in the training set as centers of the RB functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=[.3,.8,1.3]#varying spread\n",
    "store=np.zeros((3,200))#storing the loss\n",
    "store_acc=np.zeros((3,200))#storing the accuracy\n",
    "z1=-1\n",
    "for i in sp:#for each spread we calculate the loss and accuracy\n",
    "    z1=z1+1\n",
    "    weights = np.random.randn(352,1)# Weight initialization\n",
    "    bias = np.random.randn(1)# Bias initialization\n",
    "    for epoch in range(200):\n",
    "        inp_dist=dist(X_train,X_train)# calculating distance from hidden layer neurons to input\n",
    "        feature_set=rbf(inp_dist,i)#passing it to the rbf function\n",
    "\n",
    "        # FeedForward\n",
    "        fw_pass = np.dot(feature_set, weights) + bias\n",
    "        z = sigmoid(fw_pass)\n",
    "        # Error calculation\n",
    "        error = z - labels\n",
    "        e1=(z-labels)*(z-labels)\n",
    "        # Backpropagation\n",
    "        weights -= lr * np.dot(feature_set.T, error)\n",
    "        # print(z_delta,'z_delta')\n",
    "        for num in error:\n",
    "            bias -= lr * num\n",
    "\n",
    "        #Accuracy calculation using the test datset\n",
    "        y_pred=fwd(X_test,X_train,weights,bias,i)\n",
    "        a1=acc(y_pred,label_conv(y_test))\n",
    "        print('Epoch num :',epoch,' Spread value ',i,' Error :',(e1.sum())/352,' Accuracy :',a1)\n",
    "        #storing loss and accuracy\n",
    "        store[int(z1),epoch]=(e1.sum())/352\n",
    "        store_acc[int(z1),epoch]=a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe conversion\n",
    "import matplotlib.pyplot as plt\n",
    "store_df=pd.DataFrame(store.T,columns=['Spread =0.3','Spread =0.8','Spread =1.3'])\n",
    "store_acc_df=pd.DataFrame(store_acc.T,columns=['Accuracy for Spread =0.3','Accuracy for Spread =0.8','Accuracy for Spread =1.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting loss vs iterations\n",
    "store_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Mean square error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_acc_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance results (mean square error)\n",
    "From the figure, we can see that mean square error is decreasing differently for different spread parameter. For spread = 0.8, the loss is decreasing very fast with each iteration. While for spread = 1.2 it took around 90 iterations to reach the error value of other two spread parameters. We want to reduce the error value as fast as possible since it reduces the training time. So, for spread =0.8, we got best performance in terms of the rate of error value reduction. Spread = 0.3 is giving the second best performance and spread = 1.3 gave the worst performance. Besides, from the figure we can see that a lot of fluctuation of error value in case of spread = 1.3 compared to other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the RBF NN, using this time only 150 centers, Randomly select the centers from the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting 150 centers\n",
    "data_rand_new=np.zeros((150,3))\n",
    "for i in range(150):\n",
    "    out=0\n",
    "    x_i=random.choice(t[:,0])\n",
    "    x_j=random.choice(t[:,1])\n",
    "    k_val=x_i*x_i+x_j*x_j\n",
    "    if k_val<=1:\n",
    "        out=+1\n",
    "    elif k_val>1:\n",
    "        out=-1\n",
    "    data_rand_new[i,0]=x_i\n",
    "    data_rand_new[i,1]=x_j\n",
    "    data_rand_new[i,2]=out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly selected centers from the input data\n",
    "hidden_1=data_rand_new[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly selected center points\n",
    "hidden_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=[.4,.8,1.2]#varying spread\n",
    "store=np.zeros((3,200))#storing the loss\n",
    "store_acc=np.zeros((3,200))#storing the accuracy\n",
    "z1=-1\n",
    "for i in sp:#for each spread we calculate the loss and accuracy\n",
    "    z1=z1+1\n",
    "    weights = np.random.randn(150,1)# Weight initialization\n",
    "    bias = np.random.randn(1)# Bias initialization\n",
    "    for epoch in range(200):\n",
    "        inp_dist=dist(X_train,hidden_1)# calculating distance from hidden layer neurons to input\n",
    "        feature_set=rbf(inp_dist,i)#passing it to the rbf function\n",
    "\n",
    "        #Feedforward\n",
    "        fw_pass = np.dot(feature_set.T, weights) + bias\n",
    "        z = sigmoid(fw_pass)\n",
    "\n",
    "\n",
    "        # Error calculation\n",
    "        error = z - labels\n",
    "        e1=np.abs(z-labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        weights -= lr * np.dot(feature_set, error)\n",
    "        # print(z_delta,'z_delta')\n",
    "        for num in error:\n",
    "            bias -= lr * num\n",
    "\n",
    "        #acc\n",
    "        y_pred=fwd(X_test,hidden_1,weights,bias,i)\n",
    "        a1=acc(y_pred,label_conv(y_test))\n",
    "        print('Epoch num :',epoch,' Spread value ',i,' Error :',(e1.sum())/352,' Accuracy :',a1)\n",
    "        store[int(z1),epoch]=(e1.sum())/352\n",
    "        store_acc[int(z1),epoch]=a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.DataFrame(store.T,columns=['Spread =0.3','Spread =0.8','Spread =1.3'])\n",
    "store_acc_df=pd.DataFrame(store_acc.T,columns=['Accuracy for Spread =0.3','Accuracy for Spread =0.8','Accuracy for Spread =1.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Mean square error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_acc_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the RBF NN, using  only 150 centers  by K-Means algorithm to find the centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding out the kmeans center\n",
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters=150)\n",
    "km.fit(X_train)\n",
    "K_cent=km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans centers\n",
    "print(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=[.3,.8,1.3]#varying spread\n",
    "store=np.zeros((3,200))#storing the loss\n",
    "store_acc=np.zeros((3,200))#storing the accuracy\n",
    "z1=-1\n",
    "for i in sp:\n",
    "    z1=z1+1\n",
    "    weights = np.random.randn(150,1)# Weight initialization\n",
    "    bias = np.random.randn(1)# Bias initialization\n",
    "    for epoch in range(200):\n",
    "        inp_dist=dist(X_train,K_cent)# calculating distance from hidden layer neurons to input\n",
    "        feature_set=rbf(inp_dist,i)#passing it to the rbf function\n",
    "\n",
    "        # Forwardpropagation\n",
    "        fw_pass = np.dot(feature_set.T, weights) + bias\n",
    "        z = sigmoid(fw_pass)\n",
    "        #error calculation\n",
    "        error = z - labels\n",
    "        e1=np.abs(z-labels)\n",
    "        # backpropagation step 2\n",
    "        weights -= lr * np.dot(feature_set, error)\n",
    "        # print(z_delta,'z_delta')\n",
    "        for num in error:\n",
    "            bias -= lr * num\n",
    "\n",
    "        #accuracy\n",
    "        y_pred=fwd(X_test,K_cent,weights,bias,i)\n",
    "        a1=acc(y_pred,label_conv(y_test))\n",
    "        print('Epoch num :',epoch,' Spread value ',i,' Error :',(e1.sum())/352,' Accuracy :',a1)\n",
    "\n",
    "        store[int(z1),epoch]=(e1.sum())/352\n",
    "        store_acc[int(z1),epoch]=a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.DataFrame(store.T,columns=['Spread =0.3','Spread =0.8','Spread =1.3'])\n",
    "store_acc_df=pd.DataFrame(store_acc.T,columns=['Accuracy for Spread =0.3','Accuracy for Spread =0.8','Accuracy for Spread =1.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Mean square error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_acc_df.plot.line()\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of network performance between Question 3.1 and Question 3.2 for spread = 0.8\n",
    "### Accuracy of various networks\n",
    "<ul>\n",
    "<li>Network(1) with 352 points in the training set as centers of the RB function has an accuracy of 98.875% after 200 epochs</li>\n",
    "<li>Network(2) with 150 points as centers of the RB function (Radomly selected) has an accuracy of 98.875% after 200 epochs  </li>\n",
    "<li>Network(3) with 150 points as centers of the RB function (selected by kmeans) has an accuracy of 98.875% after 200 epochs</li>\n",
    "</ul>\n",
    "Since all of the three processes are giving the same accuracy after 200 epochs, we tried to find out which of these three processes is giving highest accuracy with less number of epochs.\n",
    "\n",
    "### Highest accuracy with lowest possible training time\n",
    "\n",
    "<ul>\n",
    "<li>Network(1) with 352 points in the training set as centers of the RB function has a stable accuracy of 98.875% after 10 epochs</li>\n",
    "<li>Network(2) with 150 points as centers of the RB function (Radomly selected) has a stable accuracy of 98.875% after 15 epochs  </li>\n",
    "<li>Network(3) with 150 points as centers of the RB function (selected by kmeans) has a stable accuracy of 98.875% after 9 epochs</li>\n",
    "</ul>\n",
    "\n",
    "So, from the above discussion we can say that network(3) is giving the best perfomance interms of accuracy taking the lowest possible training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
